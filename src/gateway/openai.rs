//! OpenAI å…¼å®¹ API ç«¯ç‚¹ ğŸ¤–
//! 
//! @å¦®å¨… çš„ OpenAI æ ¼å¼é€‚é…å±‚å–µ
//! 
//! ç«¯ç‚¹:
//! - POST /v1/chat/completions (OpenAI å…¼å®¹)
//! - GET /v1/models
//! - GET /v1/tools

use axum::{
    extract::{State, Request},
    http::StatusCode,
    response::{IntoResponse, Json, Response},
    routing::{get, post},
    Router,
};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tracing::{debug, info};

use super::server::GatewayState;

/// ğŸ”’ SAFETY: OpenAI Chat è¯·æ±‚å–µ
#[derive(Debug, Deserialize)]
pub struct ChatCompletionRequest {
    /// æ¨¡å‹åç§°
    pub model: String,
    /// æ¶ˆæ¯åˆ—è¡¨
    pub messages: Vec<Message>,
    /// æ¸©åº¦ (0.0-2.0)
    #[serde(default = "default_temperature")]
    pub temperature: f32,
    /// æœ€å¤§ Token æ•°
    #[serde(default)]
    pub max_tokens: Option<u32>,
    /// æµå¼è¾“å‡º
    #[serde(default)]
    pub stream: bool,
}

fn default_temperature() -> f32 { 0.7 }

/// ğŸ”’ SAFETY: æ¶ˆæ¯ç»“æ„å–µ
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Message {
    /// è§’è‰² (system/user/assistant)
    pub role: String,
    /// å†…å®¹
    pub content: String,
}

/// ğŸ”’ SAFETY: Chat å“åº”å–µ
#[derive(Debug, Serialize)]
pub struct ChatCompletionResponse {
    pub id: String,
    pub object: String,
    pub created: u64,
    pub model: String,
    pub choices: Vec<Choice>,
    pub usage: Usage,
}

#[derive(Debug, Serialize)]
pub struct Choice {
    pub index: u32,
    pub message: Message,
    pub finish_reason: String,
}

#[derive(Debug, Serialize)]
pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

/// ğŸ”’ SAFETY: Models å“åº”å–µ
#[derive(Debug, Serialize)]
pub struct ModelsResponse {
    pub object: String,
    pub data: Vec<ModelInfo>,
}

#[derive(Debug, Serialize)]
pub struct ModelInfo {
    pub id: String,
    pub object: String,
    pub owned_by: String,
}

/// ğŸ”’ SAFETY: å·¥å…·å“åº”å–µ
#[derive(Debug, Serialize)]
pub struct ToolsResponse {
    pub tools: Vec<ToolInfo>,
}

#[derive(Debug, Serialize)]
pub struct ToolInfo {
    pub name: String,
    pub description: String,
}

/// ğŸ”’ SAFETY: Chat Completions ç«¯ç‚¹å–µ
pub async fn chat_completions(
    State(state): State<Arc<GatewayState>>,
    Json(req): Json<ChatCompletionRequest>,
) -> Result<Json<ChatCompletionResponse>, (StatusCode, String)> {
    info!("Chat request: model={}, messages={}", req.model, req.messages.len());
    
    // TODO: å®é™…è°ƒç”¨ Agent å¤„ç†
    // ç›®å‰è¿”å›æ¨¡æ‹Ÿå“åº”
    
    let response = ChatCompletionResponse {
        id: format!("chatcmpl-{}", uuid::Uuid::new_v4()),
        object: "chat.completion".to_string(),
        created: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs(),
        model: req.model.clone(),
        choices: vec![Choice {
            index: 0,
            message: Message {
                role: "assistant".to_string(),
                content: "å–µ~ NekoClaw API å·²å¯åŠ¨ï¼è¿™æ˜¯æ¨¡æ‹Ÿå“åº”å–µã€‚".to_string(),
            },
            finish_reason: "stop".to_string(),
        }],
        usage: Usage {
            prompt_tokens: 10,
            completion_tokens: 20,
            total_tokens: 30,
        },
    };
    
    Ok(Json(response))
}

/// ğŸ”’ SAFETY: åˆ—å‡ºæ¨¡å‹å–µ
pub async fn list_models() -> Json<ModelsResponse> {
    Json(ModelsResponse {
        object: "list".to_string(),
        data: vec![
            ModelInfo {
                id: "z-ai/glm5".to_string(),
                object: "model".to_string(),
                owned_by: "nvidia".to_string(),
            },
            ModelInfo {
                id: "deepseek-ai/deepseek-v3.2".to_string(),
                object: "model".to_string(),
                owned_by: "deepseek".to_string(),
            },
        ],
    })
}

/// ğŸ”’ SAFETY: åˆ—å‡ºå·¥å…·å–µ
pub async fn list_tools() -> Json<ToolsResponse> {
    Json(ToolsResponse {
        tools: vec![
            ToolInfo {
                name: "fs_read".to_string(),
                description: "è¯»å–æ–‡ä»¶å†…å®¹".to_string(),
            },
            ToolInfo {
                name: "fs_write".to_string(),
                description: "å†™å…¥æ–‡ä»¶å†…å®¹".to_string(),
            },
            ToolInfo {
                name: "echo".to_string(),
                description: "å›æ˜¾æ¶ˆæ¯".to_string(),
            },
        ],
    })
}

/// ğŸ”’ SAFETY: åˆ›å»º OpenAI å…¼å®¹è·¯ç”±å–µ
pub fn create_openai_routes() -> Router<Arc<GatewayState>> {
    Router::new()
        .route("/v1/chat/completions", post(chat_completions))
        .route("/v1/models", get(list_models))
        .route("/v1/tools", get(list_tools))
}
